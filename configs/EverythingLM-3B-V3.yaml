model_name: EverythingLM-3B-V3
base_model: cerebras/btlm-3b-8k-base
model_family: llama
model_context_window: 8096
data:
  type: vicuna
  dataset: totally-not-an-llm/EverythingLM-data-V2-sharegpt
lora:
  r: 8
  lora_alpha: 32
  target_modules:
  - c_attn
  - c_proj
  lora_dropout: 0.10
  bias: none
  task_type: CAUSAL_LM
trainer:
  batch_size: 1
  gradient_accumulation_steps: 1
  warmup_steps: 50
  num_train_epochs: 2
  learning_rate: 0.0002
  logging_steps: 20
trainer_output_dir: trainer_outputs/
model_output_dir: models/
