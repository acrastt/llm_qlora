model_name: Codu-Mini-3B
base_model: openlm-research/open_llama_3b_v2
model_context_window: 4096
data:
  type: text
  dataset: rombodawg/LMCT_V3_MINI_1000_guanaco_format
lora:
  r: 16
  lora_alpha: 8
  target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - lm_head
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
trainer:
  batch_size: 4
  gradient_accumulation_steps: 4
  warmup_steps: 100
  num_train_epochs: 3
  learning_rate: 0.001
  logging_steps: 100
trainer_output_dir: trainer_outputs/
model_output_dir: models/
