model_name: Griffin-3B
base_model: openlm-research/open_llama_3b_v2
model_family: llama  
model_context_window: 2048
data:
  type: vicuna
  dataset: LDJnr/Puffin
lora:
  r: 8
  lora_alpha: 32
  target_modules:  # modules for which to train lora adapters
  - q_proj
  - k_proj
  - v_proj
  lora_dropout: 0.10
  bias: none
  task_type: CAUSAL_LM
trainer:
  batch_size: 4
  gradient_accumulation_steps: 1
  warmup_steps: 100
  num_train_epochs: 1
  learning_rate: 0.0001
  logging_steps: 10
trainer_output_dir: trainer_outputs/
model_output_dir: models/  # model saved in {model_output_dir}/{model_name}
